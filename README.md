# ğŸš€ Dockerized Stock Market ETL Pipeline

*Automated end-to-end ETL pipeline using Airflow, Python, PostgreSQL & Docker*

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" />
  <img src="https://img.shields.io/badge/Airflow-2.9.0-orange.svg" />
  <img src="https://img.shields.io/badge/PostgreSQL-16-blue.svg" />
  <img src="https://img.shields.io/badge/Docker-Compose-2496ED.svg?logo=docker&logoColor=white" />
  <img src="https://img.shields.io/badge/ETL-Pipeline-green.svg" />
  <img src="https://img.shields.io/badge/Made%20With-%E2%9D%A4-red.svg" />
</p>

---

## ğŸ“š Table of Contents

* [ğŸ“˜ Overview](#-overview)
* [ğŸ— Architecture](#-architecture)
* [ğŸ“ Project Structure](#-project-structure)
* [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
* [ğŸ”‘ Environment Variables](#-environment-variables)
* [â–¶ï¸ How to Run the Pipeline](#ï¸-how-to-run-the-pipeline)
* [ğŸ—„ Database Table](#-database-table)
* [ğŸ ETL Logic](#-etl-logic)
* [âš ï¸ Error Handling & Robustness](#ï¸-error-handling--robustness)
* [â± Airflow DAG Scheduling](#-airflow-dag-scheduling)
* [ğŸ³ Docker Services](#-docker-services)
* [ğŸš€ Optional Enhancements](#-optional-enhancements)
* [ğŸ“¸ Screenshots (Optional)](#-screenshots-optional)
* [âœ… Assignment Requirements Checklist](#-assignment-requirements-checklist)

---

# ğŸ“˜ Overview

This project implements a **Dockerized data pipeline** using:

* **Apache Airflow** for workflow orchestration
* **Python** for extraction & transformation
* **PostgreSQL** for persistent storage
* **Docker Compose** for containerized setup

The goal is to fetch **daily stock prices** from the **Alpha Vantage API**, parse the JSON response, and load it into a PostgreSQL database â€” all orchestrated via Airflow.

This fulfills all assignment requirements, including:
âœ” API extraction
âœ” JSON parsing
âœ” Database loading
âœ” Error handling
âœ” Docker & Airflow orchestration
âœ” Environment variableâ€“based configuration
âœ” Clean GitHub-ready project structure

---

# ğŸ— Architecture

```mermaid
flowchart TD
    A[Alpha Vantage API<br>JSON Stock Data] --> B[Airflow DAG<br>Scheduler & Orchestrator]
    B --> C[Python ETL Script<br>fetch_and_load.py]
    C --> D[(PostgreSQL Database<br>stock_prices table)]
```

---

# ğŸ“ Project Structure

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ stock_pipeline_dag.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ fetch_and_load.py
â””â”€â”€ sql/
    â””â”€â”€ init.sql
```

### Folder Description

| File/Folder                 | Description                          |
| --------------------------- | ------------------------------------ |
| `docker-compose.yml`        | Runs Airflow + PostgreSQL containers |
| `airflow/dags/*.py`         | Airflow DAG definition               |
| `scripts/fetch_and_load.py` | Python ETL logic                     |
| `sql/init.sql`              | SQL script to auto-create table      |
| `.env.example`              | Template for environment variables   |
| `README.md`                 | Project documentation                |

---

# ğŸ› ï¸ Tech Stack

* **Python 3.10+**
* **Apache Airflow 2.9.0**
* **Docker & Docker Compose**
* **PostgreSQL 16**
* **Alpha Vantage API**

---

# ğŸ”‘ Environment Variables

Create `.env` based on `.env.example`

```env
POSTGRES_DB=stocksdb
POSTGRES_USER=stocksuser
POSTGRES_PASSWORD=stockspass
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

AIRFLOW__CORE__LOAD_EXAMPLES=False

ALPHA_VANTAGE_API_KEY=YOUR_API_KEY_HERE
STOCK_SYMBOL=AAPL
```

âš ï¸ **Never commit your real `.env` to GitHub.**

---

# â–¶ï¸ How to Run the Pipeline

### **1ï¸âƒ£ Clone the Repository**

```bash
git clone <your-repo-url>
cd docker-airflow-stock-pipeline
```

### **2ï¸âƒ£ Create `.env` File**

```bash
cp .env.example .env
```

Add your real API key.

### **3ï¸âƒ£ Start Docker**

```bash
docker compose up --build
```

### **4ï¸âƒ£ Access Airflow UI**

Open:
ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Login:

```
username: admin
password: admin
```

### **5ï¸âƒ£ Trigger ETL Pipeline**

1. Enable DAG: `stock_data_pipeline`
2. Click **Trigger DAG**

---

# ğŸ—„ Database Table

Created automatically using `sql/init.sql`:

| Column     | Type                    |
| ---------- | ----------------------- |
| id         | SERIAL PRIMARY KEY      |
| symbol     | VARCHAR(10)             |
| ts         | TIMESTAMP               |
| open       | NUMERIC                 |
| high       | NUMERIC                 |
| low        | NUMERIC                 |
| close      | NUMERIC                 |
| volume     | BIGINT                  |
| created_at | TIMESTAMP DEFAULT NOW() |

---

# ğŸ ETL Logic

The Python script:

* Fetches JSON stock data
* Parses required values
* Cleans data
* Loads into PostgreSQL

### Extract Example:

```python
resp = requests.get(API_URL, params=params)
```

### Transform Example:

```python
float(values.get("1. open", 0))
int(values.get("5. volume", 0))
```

### Load Example:

```python
execute_values(cur, insert_query, rows)
```

---

# âš ï¸ Error Handling & Robustness

The ETL script includes:

### 1ï¸âƒ£ API Error Handling

```python
try:
    resp.raise_for_status()
except Exception as e:
    print(f"[ERROR] API request failed: {e}")
```

### 2ï¸âƒ£ Missing Data

```python
if not time_series:
    print("[WARN] No stock data available")
```

### 3ï¸âƒ£ Skipping Bad Records

```python
except Exception:
    print("[WARN] Skipping a malformed record")
```

### 4ï¸âƒ£ Database Error Handling

```python
except psycopg2.Error as e:
    print(f"[ERROR] Database error: {e}")
```

---

# â± Airflow DAG Scheduling

Runs daily:

```python
schedule_interval="@daily"
```

Includes:

* Retries
* Retry delay
* Logging
* Single ETL task

---

# ğŸ³ Docker Services

| Service    | Purpose                                     |
| ---------- | ------------------------------------------- |
| `airflow`  | Webserver + Scheduler                       |
| `postgres` | Relational database                         |
| `python`   | Dependencies installed via requirements.txt |

---

# ğŸš€ Optional Enhancements

* Add Airflow email alerts
* Add retry backoff for API
* Add Power BI / Tableau dashboard
* Move data to DWH (Snowflake, BigQuery)
* Add monitoring with Grafana

---

# ğŸ“¸ Screenshots (Optional)

Upload images and include them like:

```markdown
![Airflow DAG](images/airflow_dag.png)
![Postgres Table](images/postgres_table.png)
```

---

# âœ… Assignment Requirements Checklist

| Requirement                      | Status |
| -------------------------------- | ------ |
| Fetch API data                   | âœ…      |
| Parse JSON structure             | âœ…      |
| Extract relevant fields          | âœ…      |
| Store into database              | âœ…      |
| Implement orchestrator (Airflow) | âœ…      |
| Use Docker                       | âœ…      |
| Handle errors                    | âœ…      |
| Use env variables                | âœ…      |
| Provide complete README          | âœ…      |
| Create GitHub repo               | âœ…      |




